# Environment mode: local or prod
APP_ENV=local

# Groq API Configuration (for budget/insights analysis in prod mode)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama-3.3-70b-versatile

# Ollama/IBM Granite Configuration (for chat/Q&A in prod mode)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=granite-code
GRANITE_MODEL_NAME=granite-code

# Application Settings
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO

# Model Routing (in prod mode):
# - /api/generate (chat) → IBM Granite via Ollama
# - /api/budget-summary → Groq
# - /api/spending-insights → Groq
# In local mode, all endpoints use Mock client

